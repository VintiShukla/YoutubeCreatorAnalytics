{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVKspOuxuAv+xm+cDD9v+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VintiShukla/YoutubeCreatorAnalytics/blob/main/content_creator_analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh6UaVFvTAxR",
        "outputId": "f2d03541-9553-4aa2-9797-1de7e75a5788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn requests flask joblib beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import joblib"
      ],
      "metadata": {
        "id": "yjXhnt2cTVO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YouTubeDataCollector:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://www.googleapis.com/youtube/v3\"\n",
        "        self.quota_used = 0\n",
        "\n",
        "    def get_trending_videos(self, region_code='US', max_results=50):\n",
        "        \"\"\"Get trending videos - perfect starting dataset\"\"\"\n",
        "        url = f\"{self.base_url}/videos\"\n",
        "        params = {\n",
        "            'part': 'statistics,snippet,contentDetails',\n",
        "            'chart': 'mostPopular',\n",
        "            'regionCode': region_code,\n",
        "            'maxResults': max_results,\n",
        "            'key': self.api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        self.quota_used += 1  # Track quota usage\n",
        "        return response.json()\n",
        "\n",
        "    def get_channel_videos(self, channel_id, max_results=50):\n",
        "        \"\"\"Get videos from specific channel\"\"\"\n",
        "        # First get uploads playlist\n",
        "        url = f\"{self.base_url}/channels\"\n",
        "        params = {\n",
        "            'part': 'contentDetails',\n",
        "            'id': channel_id,\n",
        "            'key': self.api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        uploads_playlist = response.json()['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "        # Get videos from playlist\n",
        "        url = f\"{self.base_url}/playlistItems\"\n",
        "        params = {\n",
        "            'part': 'snippet',\n",
        "            'playlistId': uploads_playlist,\n",
        "            'maxResults': max_results,\n",
        "            'key': self.api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        self.quota_used += 2\n",
        "        return response.json()\n",
        "\n",
        "    def get_video_details(self, video_ids):\n",
        "        \"\"\"Get detailed stats for multiple videos\"\"\"\n",
        "        url = f\"{self.base_url}/videos\"\n",
        "        params = {\n",
        "            'part': 'statistics,snippet,contentDetails',\n",
        "            'id': ','.join(video_ids),  # Can get up to 50 videos at once\n",
        "            'key': self.api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        self.quota_used += 1\n",
        "        return response.json()"
      ],
      "metadata": {
        "id": "AMKer3yvThVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_data_collection():\n",
        "    \"\"\"First thing to do after getting API key\"\"\"\n",
        "\n",
        "    # Replace with your actual API key\n",
        "    API_KEY = \"YOUR_API_KEY_HERE\"\n",
        "    collector = YouTubeDataCollector(API_KEY)\n",
        "\n",
        "    print(\"üöÄ Starting data collection...\")\n",
        "\n",
        "    # Test API connection\n",
        "    trending = collector.get_trending_videos(max_results=5)\n",
        "    if 'items' in trending:\n",
        "        print(\"‚úÖ API working! First video:\", trending['items'][0]['snippet']['title'])\n",
        "    else:\n",
        "        print(\"‚ùå API error:\", trending)\n",
        "        return\n",
        "\n",
        "    # Collect initial dataset\n",
        "    print(\"üìä Collecting trending videos...\")\n",
        "    all_videos = []\n",
        "\n",
        "    # Get trending from different regions\n",
        "    regions = ['US', 'GB', 'CA', 'AU', 'IN']\n",
        "    for region in regions:\n",
        "        videos = collector.get_trending_videos(region_code=region, max_results=20)\n",
        "        if 'items' in videos:\n",
        "            all_videos.extend(videos['items'])\n",
        "        time.sleep(1)  # Be respectful to API\n",
        "\n",
        "    print(f\"üìà Collected {len(all_videos)} videos\")\n",
        "    print(f\"üìä Quota used: {collector.quota_used}/10000\")\n",
        "\n",
        "    return all_videos"
      ],
      "metadata": {
        "id": "8w53U72tT4bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineer:\n",
        "    def __init__(self):\n",
        "        self.features = []\n",
        "\n",
        "    def extract_features(self, video_data):\n",
        "        \"\"\"Convert raw YouTube data into ML features\"\"\"\n",
        "\n",
        "        features_list = []\n",
        "\n",
        "        for video in video_data:\n",
        "            snippet = video['snippet']\n",
        "            stats = video['statistics']\n",
        "            content = video['contentDetails']\n",
        "\n",
        "            # Content Features\n",
        "            title_length = len(snippet['title'])\n",
        "            description_length = len(snippet.get('description', ''))\n",
        "            tags_count = len(snippet.get('tags', []))\n",
        "\n",
        "            # Timing Features\n",
        "            publish_time = pd.to_datetime(snippet['publishedAt'])\n",
        "            hour = publish_time.hour\n",
        "            day_of_week = publish_time.weekday()\n",
        "\n",
        "            # Duration parsing\n",
        "            duration = content['duration']  # Format: PT4M13S\n",
        "            duration_seconds = self.parse_duration(duration)\n",
        "\n",
        "            # Engagement Metrics (targets)\n",
        "            views = int(stats.get('viewCount', 0))\n",
        "            likes = int(stats.get('likeCount', 0))\n",
        "            comments = int(stats.get('commentCount', 0))\n",
        "\n",
        "            # Calculate engagement rate\n",
        "            engagement_rate = (likes + comments) / max(views, 1) * 100\n",
        "\n",
        "            # Feature dictionary\n",
        "            features = {\n",
        "                # Input features\n",
        "                'title_length': title_length,\n",
        "                'description_length': description_length,\n",
        "                'tags_count': tags_count,\n",
        "                'duration_seconds': duration_seconds,\n",
        "                'publish_hour': hour,\n",
        "                'publish_day': day_of_week,\n",
        "                'has_thumbnail': 'maxres' in snippet.get('thumbnails', {}),\n",
        "\n",
        "                # Target variables\n",
        "                'views': views,\n",
        "                'likes': likes,\n",
        "                'comments': comments,\n",
        "                'engagement_rate': engagement_rate,\n",
        "\n",
        "                # Metadata\n",
        "                'video_id': video['id'],\n",
        "                'channel_id': snippet['channelId'],\n",
        "                'title': snippet['title']\n",
        "            }\n",
        "\n",
        "            features_list.append(features)\n",
        "\n",
        "        return pd.DataFrame(features_list)\n",
        "\n",
        "    def parse_duration(self, duration_str):\n",
        "        \"\"\"Convert PT4M13S to seconds\"\"\"\n",
        "        import re\n",
        "\n",
        "        match = re.match(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?', duration_str)\n",
        "        if not match:\n",
        "            return 0\n",
        "\n",
        "        hours = int(match.group(1) or 0)\n",
        "        minutes = int(match.group(2) or 0)\n",
        "        seconds = int(match.group(3) or 0)\n",
        "\n",
        "        return hours * 3600 + minutes * 60 + seconds"
      ],
      "metadata": {
        "id": "NhqKbB3GUxtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EngagementPredictor:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "\n",
        "        # Remove outliers (videos with extremely high views)\n",
        "        df = df[df['views'] < df['views'].quantile(0.99)]\n",
        "\n",
        "        # Feature selection\n",
        "        feature_cols = [\n",
        "            'title_length', 'description_length', 'tags_count',\n",
        "            'duration_seconds', 'publish_hour', 'publish_day', 'has_thumbnail'\n",
        "        ]\n",
        "\n",
        "        # Target variable\n",
        "        target = 'engagement_rate'\n",
        "\n",
        "        X = df[feature_cols]\n",
        "        y = df[target]\n",
        "\n",
        "        # Handle missing values\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        self.feature_columns = feature_cols\n",
        "        return X, y\n",
        "\n",
        "    def train_model(self, X, y):\n",
        "        \"\"\"Train engagement prediction model\"\"\"\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"üìä Model Performance:\")\n",
        "        print(f\"   MAE: {mae:.4f}\")\n",
        "        print(f\"   R¬≤ Score: {r2:.4f}\")\n",
        "\n",
        "        # Feature importance\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_columns,\n",
        "            'importance': self.model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(f\"üéØ Top Features:\")\n",
        "        print(importance_df.head())\n",
        "\n",
        "        return {'mae': mae, 'r2': r2, 'feature_importance': importance_df}\n",
        "\n",
        "    def predict_engagement(self, video_features):\n",
        "        \"\"\"Predict engagement for new video\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        prediction = self.model.predict([video_features])\n",
        "        return prediction[0]\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save trained model\"\"\"\n",
        "        joblib.dump({\n",
        "            'model': self.model,\n",
        "            'feature_columns': self.feature_columns\n",
        "        }, filepath)\n",
        "        print(f\"üíæ Model saved to {filepath}\")\n"
      ],
      "metadata": {
        "id": "xF277rR9axUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXY0tg0EiqE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}